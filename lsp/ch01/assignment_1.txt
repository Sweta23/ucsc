1.1) write a simple program that will run 100% of user CPU. 

ans:
int main(int argc, char** argv)
{
    while(1) ;
    return 0;
}


1.2a) If calendar time is stored in a 32bit signed integer, in what year will
      will the time run out (ie overflow)?

ans:
      2^32 seconds after 1 January 1970 
      in tcl, this is a one-liner
      clock seconds [expr {(int(pow(2,31))}]
      Mon Jan 18 07:14:08 PM PST 2038
      so, signed 32 bit time runs out in the year 2038.

1.2b) if the process time is stored in a 32bit signed integer and the system
      counts 100 ticks per second, how many days does it take for the value
      to overflow?

ans: 
      (2^31 ticks) / (100(tick/second) * 3600(seconds/hour) * 24(hours/day)) 
      = 2^31 / (100 * 3600 * 24)
      = 248.55 days



1.3) What's wrong with the following code?

#include <stdio.h>
int main(void) 
{
    char c;
    while ( (c=getchar()) != EOF ) {
        putchar(c);
    }
}

ans:
    - missing return statement at the end of main()
    - function signature of main() is non-standard but accepted by most compilers
    - type of variable 'c' should be an int. 


1.4) Determine the maximum number of open files for a process
   a) experimentally
   b) using the sysconf() function with _SC_OPEN_MAX 
ans: see attached source code, 'prog1.c' and sample run output in transcript file. 


1.5) Determine the number of clock ticks per second on your system...
   a) using getconf command line utility
   b) using the sysconf() function.
ans: see attached source code, 'prog1.c' and sample run output in transcript file. 

